Standard Identification â€“ Deep Learning Mini Projects
======================================================

This folder contains a series of Python mini-projects developed during my final-year university course titled "Standard Identification." 
The course focused on both the mathematical foundations and the practical implementation of Deep Learning algorithms.

Across four homeworks (HW1â€“HW4), I worked on statistical classification, clustering, density estimation, dimensionality reduction, and neural network training.
The implementations use Python libraries like NumPy, scikit-learn, TensorFlow, and matplotlib. Each subproject is documented in a separate notebook or script.

--------------------------------
HW1 â€“ Bayesian Classification & Distributions
--------------------------------
This homework introduces basic classification theory, Bayesian decision rules, distance metrics, and distribution analysis.

ğŸ”¹ SubHW1 â€“ Bayesian Classifier:
- Implements a cost-sensitive Bayesian classifier using Gaussian samples.
- Computes posterior probabilities and expected classification cost.

ğŸ”¹ SubHW2a â€“ Distance Metrics:
- Defines discriminant, Euclidean, and Mahalanobis distance functions used in classification.

ğŸ”¹ SubHW2b â€“ Synthetic Dataset:
- Generates a synthetic 3-class dataset from multivariate Gaussians.
- Estimates class-wise means and covariances.
- Classifies samples using Mahalanobis distance and evaluates error rates.

ğŸ”¹ SubHW3 â€“ Beta Distribution:
- Visualizes Beta-like PDFs and computes Beta function symbolically with SymPy.

--------------------------------
HW2 â€“ Density Estimation & Perceptron/SVM Classification
--------------------------------

ğŸ”¹ SubHW2-1 â€“ Density Estimation:
- Parses and visualizes a 2D labeled dataset.
- Implements Parzen Window Estimation with Gaussian kernels for various bandwidths.
- Uses k-NN for density estimation and compares outputs.
- Trains a Naive Bayes classifier and overlays decision contours with Parzen density estimates.

ğŸ”¹ SubHW2-2 â€“ Gaussian Samples & Classifiers:
- Generates 2D Gaussian-distributed samples for two classes.
- Trains a custom Perceptron classifier (batch updates, decision boundary visualization).
- Trains an SVM classifier using scikit-learn and evaluates on train/test sets.

--------------------------------
HW3 â€“ Clustering & Dimensionality Reduction (Seeds Dataset)
--------------------------------

All sub-assignments use the same dataset (seeds_dataset.txt), which contains 210 samples of wheat kernels labeled into 3 classes,
described by 7 morphological features.

ğŸ”¹ SubHW3-1 â€“ Clustering & Similarity:
- Computes Euclidean and Cosine distance matrices.
- Performs KMeans clustering (k=2 to 10), evaluates Silhouette Score.
- Computes Adjusted Rand Index over multiple runs to assess stability.

ğŸ”¹ SubHW3-2 â€“ Hierarchical Clustering:
- Applies Agglomerative Clustering with Ward linkage.
- Visualizes clustering via dendrogram.
- Evaluates clustering using Adjusted Rand Index.

ğŸ”¹ SubHW3-3 â€“ PCA & LDA:
- Applies PCA for variance retention and 2D visualization.
- Evaluates reconstruction error for 1â€“7 PCA components.
- Applies LDA to project data into a supervised 2D space.
- Analyzes feature contribution per LDA component.
- Visualizes informative feature pairs (e.g., feature index 3 vs. 4).

ğŸ§  Key Concepts:
- Similarity metrics
- KMeans and Hierarchical clustering
- Silhouette Score & ARI
- PCA (unsupervised) and LDA (supervised)
- Feature contribution & visualization

--------------------------------
HW4 â€“ Neural Network Classification (Iris Dataset)
--------------------------------

Uses the classic Iris dataset (150 samples, 4 features, 3 classes).

ğŸ”¹ Part A â€“ Dataset Splitting:
- 80/20 train-test split using scikit-learn.

ğŸ”¹ Part B â€“ Preprocessing:
- Label encoding of class names to numeric labels.
- Feature standardization (mean = 0, std = 1).

ğŸ”¹ Part C â€“ Neural Network (Sigmoid):
- 2-layer NN with 30 sigmoid neurons in hidden layer.
- Output layer with softmax for multiclass classification.
- Trained with Adam optimizer, 30 epochs, batch size 32.

ğŸ”¹ Part D â€“ Evaluation:
- Plots accuracy over epochs.
- Computes confusion matrix on test data.

ğŸ”¹ Part E â€“ Neural Network (ReLU):
- Retrains the same network with ReLU activation.

ğŸ”¹ Part F â€“ Model Comparison:
- Compares test accuracy, accuracy plots, and confusion matrices between Sigmoid and ReLU models.

--------------------------------

ğŸ—‚ï¸ Folder Structure:
- Each homework folder (HW1, HW2, HW3, HW4) contains:
  - Jupyter Notebooks (.ipynb) or Python scripts
  - Supporting datasets or generated files
  - A short description.txt of the assignment

This project collection reflects my learning progress in statistical and neural network-based classification, and my ability to implement and analyze machine learning workflows from scratch.
