HW2 – Density Estimation and Classification

This project is part of the second homework (HW2) from the course Standard Identification with Deep Learning.
It is split into two subparts:

SubHW2-1: Density Estimation and Classification
Includes 4 key components that build progressively:

(I) Dataset Parsing and Visualization

Loads a labeled 2D dataset from a .txt file (features x1, x2, and class label).
Visualizes the dataset using a 2D scatter plot.
Parses the data manually into x1_values, x2_values, and class_labels for later use.

(II) Parzen Window Density Estimation

Implements Parzen window estimation using a Gaussian kernel.
Plots estimated class-conditional densities in 3D (scatter and surface views).
Compares different bandwidths (h = 0.1, 0.3, 0.7).

(III) k-NN Density Estimation

Uses k-Nearest Neighbors for density estimation.
Visualizes results in 3D for both individual samples and over a 2D grid.
Demonstrates the impact of different k values (k = 3, 10, 30).

(IV) Naive Bayes + Parzen Contours

Trains a Gaussian Naive Bayes classifier.
Visualizes decision boundaries.
Overlays Parzen window density contours for class ω₁ using various h values.

============================================================================

SubHW2-2: Multivariate Gaussian Samples and Classification
Focuses on classification using synthetic data:

- Data Generation

Creates 2D samples for two classes from multivariate normal distributions.
Visualizes them to check class overlap.

A) Perceptron Classifier

Assigns class labels and shuffles the dataset.
Implements a custom Perceptron class.
Trains using batch updates and plots the decision boundary.
Computes and displays training accuracy.

B) SVM Classifier (Linear)

Uses scikit-learn’s SVC with a linear kernel.
Splits data into train/test sets.
Trains the model and computes both train/test accuracy.
Plots the SVM decision boundary on top of the sample scatter plot.

All code and results are executed in Jupyter Notebooks.
